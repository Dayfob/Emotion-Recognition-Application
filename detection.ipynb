{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T05:22:08.701937Z",
     "start_time": "2024-08-16T05:22:08.698417Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyaudio\n",
    "import librosa\n",
    "import time\n",
    "from IPython.display import clear_output"
   ],
   "id": "1249735fabd24ca",
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-16T05:13:15.759543Z",
     "start_time": "2024-08-16T05:13:15.743413Z"
    }
   },
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_labels, d_model=128, nhead=8, num_encoder_layers=2, dim_feedforward=512, dropout=0.3):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        # Linear projection of input features\n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        \n",
    "        # Positional encoding (can be dynamically sized)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, d_model))\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc_out = nn.Linear(d_model, num_labels)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Ensure input has shape [batch_size, seq_len, input_dim]\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(1)  # Add seq_len dimension if it's missing\n",
    "\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        # Project the input features\n",
    "        x = self.input_proj(x)\n",
    "\n",
    "        # Add positional encoding\n",
    "        positional_encoding = self.positional_encoding.unsqueeze(0).expand(batch_size, seq_len, -1)\n",
    "        x += positional_encoding\n",
    "        \n",
    "        # Pass through the transformer encoder\n",
    "        x = self.transformer_encoder(x)\n",
    "        \n",
    "        # Aggregate features (mean pooling)\n",
    "        x = x.mean(dim=1)  # Average pooling over the sequence length\n",
    "        \n",
    "        # Pass through the output layer\n",
    "        x = self.fc_out(x)\n",
    "        \n",
    "        # Sigmoid activation for multi-label classification\n",
    "        return self.sigmoid(x)\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T05:13:16.333216Z",
     "start_time": "2024-08-16T05:13:16.284496Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the model\n",
    "model = TransformerModel(input_dim=170, num_labels=7)\n",
    "model.load_state_dict(torch.load('model/transformer_model_2.pth'))\n",
    "model.eval()"
   ],
   "id": "d842813e98d04190",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (input_proj): Linear(in_features=170, out_features=128, bias=True)\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.3, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.3, inplace=False)\n",
       "        (dropout2): Dropout(p=0.3, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc_out): Linear(in_features=128, out_features=7, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T05:13:24.315360Z",
     "start_time": "2024-08-16T05:13:24.296065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import librosa.feature\n",
    "\n",
    "def extract_zcr(audio):\n",
    "    zcr = librosa.feature.zero_crossing_rate(y=audio)\n",
    "    zcr_mean = np.mean(zcr.T, axis=0)\n",
    "    # print(\"extract_zcr\", zcr.shape, zcr_mean.shape)\n",
    "    return zcr_mean\n",
    "\n",
    "def extract_chroma(audio, sr):\n",
    "    chroma = librosa.feature.chroma_stft(S=audio, sr=sr)\n",
    "    chroma_mean = np.mean(chroma.T, axis=0)\n",
    "    # print(\"extract_chroma\", chroma.shape, chroma_mean.shape)\n",
    "    return chroma_mean\n",
    "\n",
    "def extract_mfccs(audio, sr):\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sr)\n",
    "    mfccs_mean = np.mean(mfccs.T, axis=0)\n",
    "    # print(\"extract_mfccs\", mfccs.shape, mfccs_mean.shape)\n",
    "    return mfccs_mean\n",
    "\n",
    "def extract_spectral_contrast(audio, sr):\n",
    "    spectral_contrast = librosa.feature.spectral_contrast(y=audio, sr=sr)\n",
    "    spectral_contrast_mean = np.mean(spectral_contrast.T, axis=0)\n",
    "    # print(\"extract_spectral_contrast\", spectral_contrast.shape, spectral_contrast_mean.shape)\n",
    "    return spectral_contrast_mean\n",
    "\n",
    "def extract_spectral_rolloff(audio, sr):\n",
    "    spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr)\n",
    "    spectral_rolloff_mean = np.mean(spectral_rolloff.T, axis=0)\n",
    "    # print(\"extract_spectral_rolloff\", spectral_rolloff.shape, spectral_rolloff_mean.shape)\n",
    "    return spectral_rolloff_mean\n",
    "\n",
    "def extract_rmse(audio):\n",
    "    rmse = librosa.feature.rms(y=audio)\n",
    "    rmse_mean = np.mean(rmse.T, axis=0)\n",
    "    # print(\"extract_rmse\", rmse.shape, rmse_mean.shape)\n",
    "    return rmse_mean\n",
    "\n",
    "def extract_mel_spectrogram(audio, sr):\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=audio, sr=sr)\n",
    "    mel_spectrogram_mean = np.mean(mel_spectrogram.T, axis=0)\n",
    "    # print(\"extract_mel_spectrogram\", mel_spectrogram.shape, mel_spectrogram_mean.shape)\n",
    "    return mel_spectrogram_mean\n",
    "\n",
    "def extract_features(data, sample_rate):\n",
    "    result = np.array([])\n",
    "    \n",
    "    # ZCR\n",
    "    zcr = extract_zcr(data)\n",
    "    result=np.hstack((result, zcr)) # stacking horizontally\n",
    "\n",
    "    # Chroma_stft\n",
    "    stft = np.abs(librosa.stft(data))\n",
    "    chroma_stft = extract_chroma(stft, sample_rate)\n",
    "    result = np.hstack((result, chroma_stft)) # stacking horizontally\n",
    "\n",
    "    # MFCC\n",
    "    mfcc = extract_mfccs(data, sample_rate)\n",
    "    result = np.hstack((result, mfcc)) # stacking horizontally\n",
    "\n",
    "    # Spectral_contrast\n",
    "    spectral_contrast = extract_spectral_contrast(data, sample_rate)\n",
    "    result = np.hstack((result, spectral_contrast)) # stacking horizontally\n",
    "\n",
    "    # Spectral_rolloff\n",
    "    spectral_rolloff = extract_spectral_rolloff(data, sample_rate)\n",
    "    result = np.hstack((result, spectral_rolloff)) # stacking horizontally\n",
    "\n",
    "    # RMS\n",
    "    rms = extract_rmse(data)\n",
    "    result = np.hstack((result, rms)) # stacking horizontally\n",
    "\n",
    "    # Mel_spectrogram\n",
    "    mel_spectrogram = extract_mel_spectrogram(data, sample_rate)\n",
    "    result = np.hstack((result, mel_spectrogram)) # stacking horizontally\n",
    "    \n",
    "    return result\n",
    "\n",
    "def get_features(path):\n",
    "    data, sample_rate = librosa.load(path)\n",
    "    \n",
    "    features = extract_features(data, sample_rate)\n",
    "    result = np.array(features)\n",
    "    \n",
    "    return result"
   ],
   "id": "63978353da956698",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T05:35:28.427960Z",
     "start_time": "2024-08-16T05:33:37.728719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set up audio parameters\n",
    "FORMAT = pyaudio.paFloat32\n",
    "CHANNELS = 1\n",
    "RATE = 22050  # Sample rate\n",
    "CHUNK = 4096*2  # Number of frames per buffer\n",
    "\n",
    "# Initialize the audio stream\n",
    "p = pyaudio.PyAudio()\n",
    "stream = p.open(format=FORMAT,\n",
    "                channels=CHANNELS,\n",
    "                rate=RATE,\n",
    "                input=True,\n",
    "                frames_per_buffer=CHUNK)\n",
    "\n",
    "print(\"Listening...\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # Read audio data from the microphone\n",
    "        data = np.frombuffer(stream.read(CHUNK), dtype=np.float32)\n",
    "        \n",
    "        # Extract features (MFCCs)\n",
    "        features = extract_features(data, RATE)\n",
    "        \n",
    "        # Prepare features for the model\n",
    "        features_tensor = torch.tensor(features, dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "        # Get model predictions\n",
    "        with torch.no_grad():\n",
    "            probabilities = model(features_tensor)\n",
    "        \n",
    "        # Convert to percentages\n",
    "        percentages = probabilities.numpy() * 100\n",
    "        \n",
    "        # Display the result\n",
    "        print(\"Predicted Probabilities: \", \n",
    "              \"{:>7.2f}%\".format(percentages.flatten()[0]),\n",
    "              \"{:>7.2f}%\".format(percentages.flatten()[1]),\n",
    "              \"{:>7.2f}%\".format(percentages.flatten()[2]),\n",
    "              \"{:>7.2f}%\".format(percentages.flatten()[3]),\n",
    "              \"{:>7.2f}%\".format(percentages.flatten()[4]),\n",
    "              \"{:>7.2f}%\".format(percentages.flatten()[5]),\n",
    "              \"{:>7.2f}%\".format(percentages.flatten()[6]), end=\"\\r\"\n",
    "              )\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopping...\")\n",
    "finally:\n",
    "    # Close the stream\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "    \n",
    "# \"angry\" \"disgust\" \"fear\" \"happy\" \"neutral+calm\" \"sad\" \"surprised\""
   ],
   "id": "aada4aaf65623f20",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening...\n",
      "Stopping...robabilities:     4.94%   20.77%    1.71%    0.72%    1.80%    3.12%   70.04%\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "dd30ea745928e173"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
